{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation\n",
    "\n",
    "Ã€ rendre pour le 15/03/2024\n",
    "\n",
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./.venv/lib/python3.10/site-packages (2.1.2)\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.10/site-packages (0.16.2)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.10/site-packages (3.8.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in ./.venv/lib/python3.10/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.10/site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.venv/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.venv/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.venv/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.venv/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in ./.venv/lib/python3.10/site-packages (from torch) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in ./.venv/lib/python3.10/site-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (from torchvision) (1.26.3)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.10/site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.10/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.10/site-packages (from matplotlib) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.10/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->torchvision) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests->torchvision) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->torchvision) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->torchvision) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision matplotlib\n",
    "TRAIN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000 elements: ['2c45b152f1' '3cb59a4fdc' 'e185ab5dc1' ... '26527458de' '25fb3a895a'\n",
      " 'f30c36bf6b']\n"
     ]
    }
   ],
   "source": [
    "with open('dataset/train.csv') as f:\n",
    "    ids = np.loadtxt(f, delimiter=',', skiprows=1, usecols=(0,), dtype=str)\n",
    "    print(ids.size, \"elements:\", ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, padding):\n",
    "        self.padding = padding\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        \"\"\"Encoder path\"\"\"\n",
    "        self.conv11 = nn.Conv2d(3, 16, 3, padding=padding)\n",
    "        self.conv12 = nn.Conv2d(16, 16, 3, padding=padding)\n",
    "        # Here we have c1 from the paper\n",
    "        self.max1 = nn.MaxPool2d(2, 2)\n",
    "        # Here is p1\n",
    "        self.conv21 = nn.Conv2d(16, 32, 3, padding=padding)\n",
    "        self.conv22 = nn.Conv2d(32, 32, 3, padding=padding)\n",
    "        # Here we have c2 from the paper\n",
    "        self.max2 = nn.MaxPool2d(2, 2)\n",
    "        # Here is p2\n",
    "        self.conv31 = nn.Conv2d(32, 64, 3, padding=padding)\n",
    "        self.conv32 = nn.Conv2d(64, 64, 3, padding=padding)\n",
    "        # Here we have c3 from the paper\n",
    "        self.max3 = nn.MaxPool2d(2, 2)\n",
    "        # Here is p3\n",
    "        self.conv41 = nn.Conv2d(64, 128, 3, padding=padding)\n",
    "        self.conv42 = nn.Conv2d(128, 128, 3, padding=padding)\n",
    "        # Here we have c4 from the paper\n",
    "        self.max4 = nn.MaxPool2d(2, 2)\n",
    "        # Here is p4\n",
    "        self.conv51 = nn.Conv2d(128, 256, 3, padding=padding)\n",
    "        self.conv52 = nn.Conv2d(256, 256, 3, padding=padding)\n",
    "        # Here we have c5 from the paper\n",
    "        self.max5 = nn.MaxPool2d(2, 2)\n",
    "        # Here is p5\n",
    "        \n",
    "        \"\"\"Decoder path\"\"\"\n",
    "        self.tconv6 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        # Here we have u6 from the paper\n",
    "        self.conv61 = nn.Conv2d(256, 128, 3, padding=padding)\n",
    "        self.conv62 = nn.Conv2d(128, 128, 3, padding=padding)\n",
    "        # Here we have c6 from the paper\n",
    "        self.tconv7 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        # Here we have u7 from the paper\n",
    "        self.conv71 = nn.Conv2d(128, 64, 3, padding=padding)\n",
    "        self.conv72 = nn.Conv2d(64, 64, 3, padding=padding)\n",
    "        # Here we have c7 from the paper\n",
    "        self.tconv8 = nn.ConvTranspose2d(64, 32, 2, stride=2)\n",
    "        # Here we have u8 from the paper\n",
    "        self.conv81 = nn.Conv2d(64, 32, 3, padding=padding)\n",
    "        self.conv82 = nn.Conv2d(32, 32, 3, padding=padding)\n",
    "        # Here we have c8 from the paper\n",
    "        self.tconv9 = nn.ConvTranspose2d(32, 16, 2, stride=2)\n",
    "        # Here we have u9 from the paper\n",
    "        self.conv91 = nn.Conv2d(32, 16, 3, padding=padding)\n",
    "        self.conv92 = nn.Conv2d(16, 16, 3, padding=padding)\n",
    "        # Here we have c9 from the paper\n",
    "        \n",
    "        \"\"\"Output layer\"\"\"\n",
    "        self.conv10 = nn.Conv2d(16, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Encoder path\"\"\"\n",
    "        x = self.conv11(x)\n",
    "        x = self.conv12(x)\n",
    "        c1 = x\n",
    "        x = self.max1(x)\n",
    "        # x = p1\n",
    "        x = self.conv21(x)\n",
    "        x = self.conv22(x)\n",
    "        c2 = x\n",
    "        x = self.max2(x)\n",
    "        # x = p2\n",
    "        x = self.conv31(x)\n",
    "        x = self.conv32(x)\n",
    "        c3 = x\n",
    "        x = self.max3(x)\n",
    "        # x = p3\n",
    "        x = self.conv41(x)\n",
    "        x = self.conv42(x)\n",
    "        c4 = x\n",
    "        x = self.max4(x)\n",
    "        # x = p4\n",
    "        x = self.conv51(x)\n",
    "        x = self.conv52(x)\n",
    "        # C5 is not used in the decoder path\n",
    "        # there is no p5 (maxpooling)\n",
    "        \n",
    "        \"\"\"Decoder path\"\"\"\n",
    "        x = self.tconv6(x, output_size=c4.size())\n",
    "        # x = u6\n",
    "        x = torch.cat([x, c4], dim=1)\n",
    "        # x = \"u6 + c4\"\n",
    "        x = self.conv61(x)\n",
    "        x = self.conv62(x)\n",
    "        # x = c6\n",
    "        x = self.tconv7(x, output_size=c3.size())\n",
    "        # x = u7\n",
    "        x = torch.cat([x, c3], dim=1)\n",
    "        # x = \"u7 + c3\"\n",
    "        x = self.conv71(x)\n",
    "        x = self.conv72(x)\n",
    "        # x = c7\n",
    "        x = self.tconv8(x, output_size=c2.size())\n",
    "        # x = u8\n",
    "        x = torch.cat([x, c2], dim=1)\n",
    "        # x = \"u8 + c2\"\n",
    "        x = self.conv81(x)\n",
    "        x = self.conv82(x)\n",
    "        # x = c8\n",
    "        x = self.tconv9(x, output_size=c1.size())\n",
    "        # x = u9\n",
    "        x = torch.cat([x, c1], dim=1)\n",
    "        # x = \"u9 + c1\"\n",
    "        x = self.conv91(x)\n",
    "        x = self.conv92(x)\n",
    "        # x = c9\n",
    "        \n",
    "        \"\"\"Output layer\"\"\"\n",
    "        x = self.conv10(x)\n",
    "        # Some x might be out of [0, 1] range\n",
    "        return torch.clamp(x, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch\t0\tLoss:\t0.5605555183887482\t      183887482\n",
      "Epoch\t1\tLoss:\t0.5664425008296966\t      008296966\n",
      "Epoch\t2\tBatch\t1088/4000\tLoss:\t0.5487694799900052\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m masks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch_ids:\n\u001b[0;32m---> 21\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdataset/train/images/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.png\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     mask \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset/train/masks/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Conv2D wants batches, channels, height, width while plt.imread returns height, width, channels\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# (moreover we need to get rid of the alpha channel)\u001b[39;00m\n",
      "File \u001b[0;32m/data/Jangberry/Nextcloud/Documents/Cours/3A/Option/Vision par ordinateur/TD/Semantic Segmentation/.venv/lib/python3.10/site-packages/matplotlib/pyplot.py:2389\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   2385\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(matplotlib\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mimread)\n\u001b[1;32m   2386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimread\u001b[39m(\n\u001b[1;32m   2387\u001b[0m         fname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath \u001b[38;5;241m|\u001b[39m BinaryIO, \u001b[38;5;28mformat\u001b[39m: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2388\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m-> 2389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmatplotlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/Jangberry/Nextcloud/Documents/Cours/3A/Option/Vision par ordinateur/TD/Semantic Segmentation/.venv/lib/python3.10/site-packages/matplotlib/image.py:1526\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1521\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease open the URL for reading and pass the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1522\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult to Pillow, e.g. with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1523\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m``np.array(PIL.Image.open(urllib.request.urlopen(url)))``.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1524\u001b[0m         )\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m img_open(fname) \u001b[38;5;28;01mas\u001b[39;00m image:\n\u001b[0;32m-> 1526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43m_pil_png_to_float_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1527\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image, PIL\u001b[38;5;241m.\u001b[39mPngImagePlugin\u001b[38;5;241m.\u001b[39mPngImageFile) \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[1;32m   1528\u001b[0m             pil_to_array(image))\n",
      "File \u001b[0;32m/data/Jangberry/Nextcloud/Documents/Cours/3A/Option/Vision par ordinateur/TD/Semantic Segmentation/.venv/lib/python3.10/site-packages/matplotlib/image.py:1720\u001b[0m, in \u001b[0;36m_pil_png_to_float_array\u001b[0;34m(pil_png)\u001b[0m\n\u001b[1;32m   1718\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdivide(pil_png\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGBA\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m8\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m   1719\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGBA\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# RGBA.\u001b[39;00m\n\u001b[0;32m-> 1720\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdivide\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpil_png\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown PIL rawmode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrawmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/data/Jangberry/Nextcloud/Documents/Cours/3A/Option/Vision par ordinateur/TD/Semantic Segmentation/.venv/lib/python3.10/site-packages/PIL/Image.py:681\u001b[0m, in \u001b[0;36mImage.__array_interface__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    679\u001b[0m         new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtobytes(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 681\u001b[0m         new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    683\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, (\u001b[38;5;167;01mMemoryError\u001b[39;00m, \u001b[38;5;167;01mRecursionError\u001b[39;00m)):\n",
      "File \u001b[0;32m/data/Jangberry/Nextcloud/Documents/Cours/3A/Option/Vision par ordinateur/TD/Semantic Segmentation/.venv/lib/python3.10/site-packages/PIL/Image.py:753\u001b[0m, in \u001b[0;36mImage.tobytes\u001b[0;34m(self, encoder_name, *args)\u001b[0m\n\u001b[1;32m    751\u001b[0m output \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 753\u001b[0m     bytes_consumed, errcode, data \u001b[38;5;241m=\u001b[39m \u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    754\u001b[0m     output\u001b[38;5;241m.\u001b[39mappend(data)\n\u001b[1;32m    755\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errcode:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.nn import BCELoss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = UNet(padding=1).to('cuda')\n",
    "criterion = BCELoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "epoch = 0\n",
    "min_loss = float('inf')\n",
    "stagnation = 0\n",
    "\n",
    "if TRAIN:\n",
    "    while epoch < epochs:\n",
    "        epoch_loss = 0\n",
    "        for i in range(0, ids.size, batch_size):\n",
    "            batch_ids = ids[i:i+batch_size]\n",
    "            images = []\n",
    "            masks = []\n",
    "            for id in batch_ids:\n",
    "                img = plt.imread('dataset/train/images/' + id + '.png')\n",
    "                mask = plt.imread('dataset/train/masks/' + id + '.png')\n",
    "\n",
    "                # Conv2D wants batches, channels, height, width while plt.imread returns height, width, channels\n",
    "                # (moreover we need to get rid of the alpha channel)\n",
    "                img = img[:,:,:3]\n",
    "                img = np.transpose(img, (2, 0, 1))\n",
    "                \n",
    "                # We create a virtual channel for the mask\n",
    "                mask = mask[:,:,None]\n",
    "                mask = np.transpose(mask, (2, 0, 1))\n",
    "                \n",
    "                images.append(img)\n",
    "                masks.append(mask)\n",
    "            inputs = torch.tensor(np.array(images), dtype=torch.float32).to('cuda')\n",
    "            labels = torch.tensor(np.array(masks), dtype=torch.float32).to('cuda')\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.cpu().item() / (ids.size / batch_size)\n",
    "\n",
    "            print('Epoch', epoch, 'Batch', str(i) + \"/\" + str(ids.size), 'Loss:', epoch_loss * (ids.size / (i + batch_size)), end='\\r', sep='\\t')\n",
    "        print('Epoch', epoch, 'Loss:', epoch_loss, \"      \", sep='\\t')\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        if epoch_loss < min_loss * (1 + lr * 1000):\n",
    "            min_loss = epoch_loss\n",
    "            torch.save(model.state_dict(), f'model-epoch{epoch}.pth')\n",
    "            epoch += 1\n",
    "        elif epoch_loss < min_loss * (1 + lr * 100000):\n",
    "            print('Stagnating, restoring previous model and nudging a bit the weights')\n",
    "            model.load_state_dict(torch.load(f'model-epoch{epoch-1}.pth'))\n",
    "            stagnation += 1\n",
    "            for param in model.parameters():\n",
    "                param.data *= (torch.randn(param.size()).to('cuda') - 0.5) * 100 * lr + 1\n",
    "            if stagnation > 3:\n",
    "                print('Stagnating for too long, reducing learning rate from', lr, 'to', lr * 0.8)\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] *= 0.8\n",
    "                stagnation = 0\n",
    "        else:\n",
    "            print('Diverging, nudging a lot parameters')\n",
    "            model.load_state_dict(torch.load(f'model-epoch{epoch-1}.pth'))\n",
    "            for param in model.parameters():\n",
    "                param.data *= (torch.randn(param.size()).to('cuda') - 0.5) * 1000000 * lr + 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
